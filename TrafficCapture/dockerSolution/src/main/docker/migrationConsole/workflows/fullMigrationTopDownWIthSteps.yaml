apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: migration-workflow-template-pipeline
spec:
  serviceAccountName: argo-workflow-executor
  templates:
    - name: main
      inputs:
        parameters:
          - name: migration_config
            default: |
              {
                "targets": [
                  {
                    "id": "target1",
                    "endpoint": "https://target1.example.com",
                    "auth": { "type": "basic", "username": "user", "password": "pass" }
                  },
                  {
                    "id": "target2",
                    "endpoint": "https://target2.example.com",
                    "auth": { "type": "basic", "username": "user", "password": "pass" }
                  }
                ],
                "source-migration-configurations": [
                  {
                    "source": {
                      "id": "sourceA",
                      "endpoint": "https://sourceA.example.com",
                      "auth": { "type": "basic", "username": "user", "password": "pass" }
                    },
                    "static-migration-configs": [
                      {
                        "indices": ["index_1", "index_2"],
                        "metadata": {
                          "mappings": { "properties": {} },
                          "settings": {}
                        },
                        "documentBackfillConfigs": [
                          {
                            "indices": ["index_1"],
                            "query": { "match_all": {} }
                          },
                          {
                            "indices": ["index_2"],
                            "query": { "match_all": {} }
                          }
                        ]
                      }
                    ],
                    "replayer-config": {
                      "batchSize": 1000,
                      "concurrency": 4
                    }
                  },
                  {
                    "source": {
                      "id": "sourceB",
                      "endpoint": "https://sourceB.example.com",
                      "auth": { "type": "basic", "username": "user", "password": "pass" }
                    },
                    "static-migration-configs": [
                      {
                        "indices": ["index_3", "index_4"],
                        "metadata": {
                          "mappings": { "properties": {} },
                          "settings": {}
                        },
                        "documentBackfillConfigs": [
                          {
                            "indices": ["index_3", "index_4"],
                            "query": { "match_all": {} }
                          }
                        ]
                      }
                    ],
                    "replayer-config": {
                      "batchSize": 1000,
                      "concurrency": 4
                    }
                  }
                ]
              }
      dag:
        tasks:
          # Setup shared Kafka once
          - name: s01-setup-shared-kafka
            template: setup-shared-kafka

          # Extract sources for easier workflow navigation
          - name: s00-extract-sources
            template: extract-sources
            arguments:
              parameters:
                - name: migration-config
                  value: "{{inputs.parameters.migration_config}}"

          # Extract source-index-target combinations for pipeline
          - name: s00-extract-pipeline-tasks
            template: extract-pipeline-tasks
            arguments:
              parameters:
                - name: migration-config
                  value: "{{inputs.parameters.migration_config}}"

          # Extract targets for replayer step
          - name: s00-extract-targets
            template: extract-targets
            arguments:
              parameters:
                - name: migration-config
                  value: "{{inputs.parameters.migration_config}}"

          # Start capture for each source and create snapshot
          - name: s02-source-captures
            template: source-capture-and-snapshot
            arguments:
              parameters:
                - name: source-config
                  value: "{{item}}"
                - name: kafka-endpoint
                  value: "{{tasks.s01-setup-shared-kafka.outputs.parameters.result}}"
            dependencies: ["s01-setup-shared-kafka", "s00-extract-sources"]
            withParam: "{{tasks.s00-extract-sources.outputs.parameters.sources}}"

          # Process all source-index-target combinations in pipeline by source
          - name: s03-process-pipeline-by-source
            template: process-pipeline-for-source
            arguments:
              parameters:
                - name: source-snapshot
                  value: "{{item}}"
                - name: all-pipeline-tasks
                  value: "{{tasks.s00-extract-pipeline-tasks.outputs.parameters.tasks}}"
            dependencies: ["s00-extract-pipeline-tasks", "s02-source-captures"]
            withParam: "{{tasks.s02-source-captures.outputs.parameters.result}}"

          # Store results in shared ConfigMap
          - name: s03a-store-results
            template: store-results-to-configmap
            arguments:
              parameters:
                - name: process-result
                  value: "{{item.result}}"
                - name: process-id
                  value: "{{item.id}}"
            dependencies: ["s03-process-pipeline-by-source"]
            withParam: "{{tasks.s03-process-pipeline-by-source.outputs.parameters.result-with-id}}"

          # Aggregate results from ConfigMap
          - name: s03b-aggregate-results
            template: aggregate-results-from-configmap
            dependencies: ["s03a-store-results"]

          # Aggregate RFS results per target and run replayer
          - name: s04-aggregate-and-replay
            template: aggregate-and-replay
            arguments:
              parameters:
                - name: target-config
                  value: "{{item}}"
                - name: all-rfs-results
                  value: "{{tasks.s03b-aggregate-results.outputs.parameters.all-results}}"
                - name: migration-config
                  value: "{{inputs.parameters.migration_config}}"
            dependencies: ["s03b-aggregate-results", "s00-extract-targets"]
            withParam: "{{tasks.s00-extract-targets.outputs.parameters.targets}}"

    # Helper template to extract sources from configuration
    - name: extract-sources
      inputs:
        parameters:
          - name: migration-config
      script:
        image: python:3.9-alpine
        env:
          - name: MIGRATION_CONFIG
            value: "{{inputs.parameters.migration-config}}"
        command: [python]
        source: |
          import json, sys, os

          config_str = os.environ.get('MIGRATION_CONFIG')
          if not config_str:
              print("ERROR: MIGRATION_CONFIG environment variable is not set or empty")
              try:
                  config_str = '''{{inputs.parameters.migration-config}}'''
              except Exception as e:
                  print(f"Fallback failed: {e}")
                  sys.exit(1)

          try:
              config = json.loads(config_str)
              sources = config.get('source-migration-configurations', [])
          
              # Extract just the source configurations
              source_configs = []
              for source_config in sources:
                  source_configs.append({
                      "source": source_config.get("source"),
                      "replayer-config": source_config.get("replayer-config", {})
                  })

              # Write the sources to output file
              with open('/tmp/sources.json', 'w') as f:
                  f.write(json.dumps(source_configs))

              print(f"Extracted {len(source_configs)} sources")
          except json.JSONDecodeError as e:
              print(f"ERROR: Failed to parse JSON: {e}")
              print(f"First 100 chars of input: {config_str[:100] if config_str else 'None'}")
              sys.exit(1)
          except Exception as e:
              print(f"ERROR: Unexpected error: {e}")
              sys.exit(1)
      outputs:
        parameters:
          - name: sources
            valueFrom:
              path: /tmp/sources.json

    # Extract pipeline tasks - each source-index-target combination
    - name: extract-pipeline-tasks
      inputs:
        parameters:
          - name: migration-config
      script:
        image: python:3.9-alpine
        env:
          - name: MIGRATION_CONFIG
            value: "{{inputs.parameters.migration-config}}"
        command: [python]
        source: |
          import json, sys, os
          
          config_str = os.environ.get('MIGRATION_CONFIG')
          if not config_str:
              print("ERROR: MIGRATION_CONFIG environment variable is not set or empty")
              try:
                  config_str = '''{{inputs.parameters.migration-config}}'''
              except Exception as e:
                  print(f"Fallback failed: {e}")
                  sys.exit(1)
          
          try:
              config = json.loads(config_str)
              targets = config.get('targets', [])
              sources = config.get('source-migration-configurations', [])
          
              # Generate all source-index-target combinations
              pipeline_tasks = []
          
              for source_config in sources:
                  source = source_config.get('source', {})
                  source_id = source.get('id')
          
                  for static_config in source_config.get('static-migration-configs', []):
                      indices = static_config.get('indices', [])
                      metadata = static_config.get('metadata', {})
          
                      # Create metadata tasks for each index for each target
                      for index in indices:
                          for target in targets:
                              target_id = target.get('id')
          
                              task = {
                                  "source_id": source_id,
                                  "index": index,
                                  "target_id": target_id,
                                  "metadata": metadata
                              }
          
                              # Find corresponding backfill config
                              for backfill in static_config.get('documentBackfillConfigs', []):
                                  if index in backfill.get('indices', []):
                                      task["backfill_query"] = backfill.get('query', {'match_all': {}})
                                      break
          
                              if "backfill_query" not in task:
                                  # Add default query if no specific one was found
                                  task["backfill_query"] = {'match_all': {}}
          
                              pipeline_tasks.append(task)
          
              # Write tasks to output file
              with open('/tmp/pipeline_tasks.json', 'w') as f:
                  f.write(json.dumps(pipeline_tasks))
          
              print(f"Generated {len(pipeline_tasks)} pipeline tasks")
          except json.JSONDecodeError as e:
              print(f"ERROR: Failed to parse JSON: {e}")
              print(f"First 100 chars of input: {config_str[:100] if config_str else 'None'}")
              sys.exit(1)
          except Exception as e:
              print(f"ERROR: Unexpected error: {e}")
              sys.exit(1)
      outputs:
        parameters:
          - name: tasks
            valueFrom:
              path: /tmp/pipeline_tasks.json

    # Template to process all tasks for a specific source
    - name: process-pipeline-for-source
      inputs:
        parameters:
          - name: source-snapshot
          - name: all-pipeline-tasks
      dag:
        tasks:
          - name: filter-tasks
            template: filter-tasks-for-source
            arguments:
              parameters:
                - name: source-snapshot
                  value: "{{inputs.parameters.source-snapshot}}"
                - name: all-pipeline-tasks
                  value: "{{inputs.parameters.all-pipeline-tasks}}"

          - name: process-source-tasks
            template: process-pipeline-task
            arguments:
              parameters:
                - name: task-with-snapshot
                  value: "{{item}}"
            dependencies: ["filter-tasks"]
            withParam: "{{tasks.filter-tasks.outputs.parameters.filtered-tasks}}"
      outputs:
        parameters:
          - name: result
            valueFrom:
              parameter: "{{tasks.process-source-tasks.outputs.parameters.result}}"
          - name: result-with-id
            valueFrom:
              expression: "'{{=JSON.stringify({\"id\": JSON.parse(inputs.parameters.source-snapshot).source_id, \"result\": tasks.process-source-tasks.outputs.parameters.result})}}'"

    # Store results in shared ConfigMap
    - name: store-results-to-configmap
      inputs:
        parameters:
          - name: process-result
          - name: process-id
      script:
        image: python:3.9-alpine
        command: [python]
        source: |
          import json, os, time
          from kubernetes import client, config
          
          # Get the process ID and result
          process_id = os.environ.get('PROCESS_ID')
          process_result = os.environ.get('PROCESS_RESULT')
          
          print(f"Storing results for process {process_id}: {process_result[:100]}...")
          
          # Simulate storing to storage or ConfigMap
          result_file = f"/tmp/results_{process_id}.json"
          with open(result_file, 'w') as f:
              f.write(process_result)
          
          # Write success marker
          with open('/tmp/store_success.txt', 'w') as f:
              f.write('success')
        env:
          - name: PROCESS_RESULT
            value: "{{inputs.parameters.process-result}}"
          - name: PROCESS_ID
            value: "{{inputs.parameters.process-id}}"
      outputs:
        parameters:
          - name: status
            valueFrom:
              path: /tmp/store_success.txt

    # Aggregate results from ConfigMap
    - name: aggregate-results-from-configmap
      script:
        image: python:3.9-alpine
        command: [python]
        source: |
          import json, os, glob, time
          
          # Simulate reading from storage or ConfigMap
          all_results = []
          
          # In a real implementation, this would read from ConfigMap or storage
          # For demo, just list directories where results would be stored
          print("Reading all stored results...")
          
          # Demo: create some results for testing
          demo_results = [
              {"source_id": "sourceA", "target_id": "target1", "index": "index_1", "rfs_result": "rfs-index_1-target1"},
              {"source_id": "sourceA", "target_id": "target1", "index": "index_2", "rfs_result": "rfs-index_2-target1"},
              {"source_id": "sourceB", "target_id": "target1", "index": "index_3", "rfs_result": "rfs-index_3-target1"},
              {"source_id": "sourceB", "target_id": "target1", "index": "index_4", "rfs_result": "rfs-index_4-target1"},
              {"source_id": "sourceA", "target_id": "target2", "index": "index_1", "rfs_result": "rfs-index_1-target2"},
              {"source_id": "sourceA", "target_id": "target2", "index": "index_2", "rfs_result": "rfs-index_2-target2"},
              {"source_id": "sourceB", "target_id": "target2", "index": "index_3", "rfs_result": "rfs-index_3-target2"},
              {"source_id": "sourceB", "target_id": "target2", "index": "index_4", "rfs_result": "rfs-index_4-target2"}
          ]
          all_results.extend(demo_results)
          
          # Try to find any real results stored in files
          result_files = glob.glob("/tmp/results_*.json")
          print(f"Found {len(result_files)} result files")
          
          for file_path in result_files:
              try:
                  with open(file_path, 'r') as f:
                      file_content = f.read()
                      result = json.loads(file_content)
                      if isinstance(result, list):
                          all_results.extend(result)
                      else:
                          all_results.append(result)
              except Exception as e:
                  print(f"Error reading result file {file_path}: {str(e)}")
          
          print(f"Aggregated {len(all_results)} results in total")
          
          # Write to output file
          with open('/tmp/all_results.json', 'w') as f:
              f.write(json.dumps(all_results))
      outputs:
        parameters:
          - name: all-results
            valueFrom:
              path: /tmp/all_results.json

    # Helper template to filter tasks for a specific source
    - name: filter-tasks-for-source
      inputs:
        parameters:
          - name: source-snapshot
          - name: all-pipeline-tasks
      script:
        image: python:3.9-alpine
        env:
          - name: SOURCE_SNAPSHOT
            value: "{{inputs.parameters.source-snapshot}}"
          - name: ALL_PIPELINE_TASKS
            value: "{{inputs.parameters.all-pipeline-tasks}}"
        command: [python]
        source: |
          import json, sys, os, ast
          
          # Parse inputs
          try:
              source_snapshot = json.loads(os.environ.get('SOURCE_SNAPSHOT'))
          except:
              try:
                  source_snapshot = ast.literal_eval(os.environ.get('SOURCE_SNAPSHOT'))
              except:
                  print("Error parsing source snapshot")
                  sys.exit(1)
          
          try:
              all_tasks = json.loads(os.environ.get('ALL_PIPELINE_TASKS'))
          except:
              try:
                  all_tasks = ast.literal_eval(os.environ.get('ALL_PIPELINE_TASKS'))
              except:
                  print("Error parsing pipeline tasks")
                  sys.exit(1)
          
          # Get source_id from snapshot
          source_id = source_snapshot.get('source_id')
          
          # Filter tasks for this source_id only
          filtered_tasks = []
          for task in all_tasks:
              if task.get('source_id') == source_id:
                  # Create the combined task-with-snapshot object
                  combined = {
                      'task': task,
                      'snapshot': source_snapshot
                  }
                  filtered_tasks.append(combined)
          
          print(f"Found {len(filtered_tasks)} tasks for source {source_id}")
          
          # Write to output file
          with open('/tmp/filtered_tasks.json', 'w') as f:
              f.write(json.dumps(filtered_tasks))
      outputs:
        parameters:
          - name: filtered-tasks
            valueFrom:
              path: /tmp/filtered_tasks.json

    # Helper template to extract targets from configuration
    - name: extract-targets
      inputs:
        parameters:
          - name: migration-config
      script:
        image: python:3.9-alpine
        env:
          - name: MIGRATION_CONFIG
            value: "{{inputs.parameters.migration-config}}"
        command: [python]
        source: |
          import json, sys, os
          
          config_str = os.environ.get('MIGRATION_CONFIG')
          if not config_str:
              print("ERROR: MIGRATION_CONFIG environment variable is not set or empty")
              try:
                  config_str = '''{{inputs.parameters.migration-config}}'''
              except Exception as e:
                  print(f"Fallback failed: {e}")
                  sys.exit(1)
          
          try:
              config = json.loads(config_str)
              targets = config.get('targets', [])
          
              # Write the targets to output file
              with open('/tmp/targets.json', 'w') as f:
                  f.write(json.dumps(targets))
          
              print(f"Extracted {len(targets)} targets")
          except json.JSONDecodeError as e:
              print(f"ERROR: Failed to parse JSON: {e}")
              print(f"First 100 chars of input: {config_str[:100] if config_str else 'None'}")
              sys.exit(1)
          except Exception as e:
              print(f"ERROR: Unexpected error: {e}")
              sys.exit(1)
      outputs:
        parameters:
          - name: targets
            valueFrom:
              path: /tmp/targets.json

    # Setup shared Kafka template
    - name: setup-shared-kafka
      script:
        image: python:3.9-alpine
        command: [python]
        source: |
          import random, time
          
          print("Setting up shared Kafka cluster")
          
          # Return Kafka endpoint
          with open('/tmp/endpoint.txt', 'w') as f:
              f.write('kafka:9092')
          
          print("Kafka setup complete")
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/endpoint.txt

    # Combined template for source capture and snapshot
    - name: source-capture-and-snapshot
      inputs:
        parameters:
          - name: source-config
          - name: kafka-endpoint
      steps:
        # Start capture
        - - name: s01-start-capture
            template: start-capture
            arguments:
              parameters:
                - name: source-config
                  value: "{{inputs.parameters.source-config}}"
                - name: kafka-endpoint
                  value: "{{inputs.parameters.kafka-endpoint}}"

        # Create snapshot
        - - name: s02-create-snapshot
            template: create-snapshot
            arguments:
              parameters:
                - name: source-config
                  value: "{{inputs.parameters.source-config}}"
                - name: capture-id
                  value: "{{steps.s01-start-capture.outputs.parameters.result}}"
      outputs:
        parameters:
          - name: result
            valueFrom:
              parameter: "{{steps.s02-create-snapshot.outputs.parameters.result}}"

    # Start capture template
    - name: start-capture
      inputs:
        parameters:
          - name: source-config
          - name: kafka-endpoint
      script:
        image: python:3.9-alpine
        env:
          - name: SOURCE_CONFIG
            value: "{{inputs.parameters.source-config}}"
          - name: KAFKA_ENDPOINT
            value: "{{inputs.parameters.kafka-endpoint}}"
        command: [python]
        source: |
          import json, sys, os, random, time
          
          source_config = json.loads(os.environ.get('SOURCE_CONFIG'))
          kafka_endpoint = os.environ.get('KAFKA_ENDPOINT')
          
          source = source_config.get('source', {})
          source_id = source.get('id')
          endpoint = source.get('endpoint')
          
          print(f"Starting capture for source {source_id} ({endpoint}) using Kafka at {kafka_endpoint}")
          
          # Return capture ID
          capture_id = f"capture-{source_id}"
          with open('/tmp/output.txt', 'w') as f:
              f.write(capture_id)
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/output.txt

    # Create snapshot template
    - name: create-snapshot
      inputs:
        parameters:
          - name: source-config
          - name: capture-id
      script:
        image: python:3.9-alpine
        env:
          - name: SOURCE_CONFIG
            value: "{{inputs.parameters.source-config}}"
          - name: CAPTURE_ID
            value: "{{inputs.parameters.capture-id}}"
        command: [python]
        source: |
          import json, sys, os, random, time
          
          source_config = json.loads(os.environ.get('SOURCE_CONFIG'))
          capture_id = os.environ.get('CAPTURE_ID')
          
          source = source_config.get('source', {})
          source_id = source.get('id')
          
          print(f"Creating snapshot for source {source_id} from capture {capture_id}")
          
          sleep_duration = random.randint(0, 60)
          print(f"Sleeping for {sleep_duration} seconds to simulate snapshot creation time...")
          time.sleep(sleep_duration)
          print(f"Finished sleeping after {sleep_duration} seconds")
          
          # Return snapshot details
          result = {
              "source_id": source_id,
              "capture_id": capture_id,
              "snapshot": f"snapshot-{source_id}"
          }
          with open('/tmp/output.json', 'w') as f:
              f.write(json.dumps(result))
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/output.json

    # Process a single pipeline task (source-index-target combination)
    - name: process-pipeline-task
      inputs:
        parameters:
          - name: task-with-snapshot
      steps:
        # Process metadata for the index-target combination
        - - name: s01-process-metadata
            template: metadata-processing
            arguments:
              parameters:
                - name: task-with-snapshot
                  value: "{{inputs.parameters.task-with-snapshot}}"

        # Process RFS for the index-target combination
        - - name: s02-process-rfs
            template: rfs-processing
            arguments:
              parameters:
                - name: pipeline-task
                  value: "{{inputs.parameters.task-with-snapshot}}"
                - name: metadata-result
                  value: "{{steps.s01-process-metadata.outputs.parameters.result}}"
      outputs:
        parameters:
          - name: result
            valueFrom:
              parameter: "{{steps.s02-process-rfs.outputs.parameters.result}}"

    # Metadata processing template
    - name: metadata-processing
      inputs:
        parameters:
          - name: task-with-snapshot
      script:
        image: python:3.9-alpine
        env:
          - name: TASK_WITH_SNAPSHOT
            value: "{{inputs.parameters.task-with-snapshot}}"
        command: [python]
        source: |
          import json, sys, os, random, time
          
          # Get the pipeline task from the combined object
          task_with_snapshot = json.loads(os.environ.get('TASK_WITH_SNAPSHOT'))
          pipeline_task = task_with_snapshot.get('task', {})
          snapshot = task_with_snapshot.get('snapshot', {})
          
          source_id = pipeline_task.get('source_id', 'unknown')
          index = pipeline_task.get('index', 'unknown')
          target_id = pipeline_task.get('target_id', 'unknown')
          metadata = pipeline_task.get('metadata', {})
          
          # Get the snapshot information
          source_snapshot = snapshot.get('snapshot', f"snapshot-{source_id}")
          
          print(f"Processing metadata for index {index} from source {source_id} to target {target_id}")
          print(f"Using snapshot: {source_snapshot}")
          
          # Add some simulated processing time
          sleep_duration = random.randint(1, 10)
          print(f"Metadata processing for {index} - sleeping for {sleep_duration} seconds...")
          time.sleep(sleep_duration)
          print(f"Metadata processing for {index} completed after {sleep_duration} seconds")
          
          # Return metadata processing result
          result = {
              "source_id": source_id,
              "target_id": target_id,
              "index": index,
              "snapshot": source_snapshot,
              "metadata_result": f"metadata-{index}-{target_id}"
          }
          with open('/tmp/output.json', 'w') as f:
              f.write(json.dumps(result))
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/output.json

    # RFS processing template
    - name: rfs-processing
      inputs:
        parameters:
          - name: pipeline-task
          - name: metadata-result
      script:
        image: python:3.9-alpine
        env:
          - name: PIPELINE_TASK
            value: "{{inputs.parameters.pipeline-task}}"
          - name: METADATA_RESULT
            value: "{{inputs.parameters.metadata-result}}"
        command: [python]
        source: |
          import json, sys, os, random, time
          
          # Parse the pipeline task from the task-with-snapshot
          try:
              task_with_snapshot = json.loads(os.environ.get('PIPELINE_TASK'))
              if isinstance(task_with_snapshot, dict) and 'task' in task_with_snapshot:
                  pipeline_task = task_with_snapshot.get('task', {})
              else:
                  pipeline_task = task_with_snapshot  # In case it's directly the task object
          except Exception as e:
              print(f"Warning: Failed to parse pipeline task: {str(e)}")
              pipeline_task = {}
          
          # Parse metadata result
          metadata_result_str = os.environ.get('METADATA_RESULT')
          try:
              metadata_result = json.loads(metadata_result_str)
          except Exception as e:
              print(f"Warning: Failed to parse metadata result: {str(e)}")
              metadata_result = {"metadata_result": "unknown"}
          
          source_id = pipeline_task.get('source_id')
          index = pipeline_task.get('index')
          target_id = pipeline_task.get('target_id')
          query = pipeline_task.get('backfill_query', {'match_all': {}})
          
          print(f"Processing RFS for index {index} from source {source_id} to target {target_id}")
          print(f"Using query: {json.dumps(query)}")
          
          # Add some simulated processing time 
          sleep_duration = random.randint(3, 15)
          print(f"RFS processing - sleeping for {sleep_duration} seconds...")
          time.sleep(sleep_duration)
          print(f"RFS processing completed after {sleep_duration} seconds")
          
          # Return RFS processing result
          result = {
              "source_id": source_id,
              "target_id": target_id,
              "index": index,
              "rfs_result": f"rfs-{index}-{target_id}",
              "metadata_used": metadata_result.get('metadata_result'),
              "snapshot": metadata_result.get('snapshot')
          }
          with open('/tmp/output.json', 'w') as f:
              f.write(json.dumps(result))
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/output.json

    # Aggregate RFS results per target and run replayer
    - name: aggregate-and-replay
      inputs:
        parameters:
          - name: target-config
          - name: all-rfs-results
          - name: migration-config
      retryStrategy:
        limit: 3
        retryPolicy: "Always"
      script:
        image: python:3.9-alpine
        env:
          - name: TARGET_CONFIG
            value: "{{inputs.parameters.target-config}}"
          - name: ALL_RFS_RESULTS
            value: "{{inputs.parameters.all-rfs-results}}"
          - name: MIGRATION_CONFIG
            value: "{{inputs.parameters.migration-config}}"
        command: [python]
        source: |
          import json, sys, os, random, time
          
          target_config = json.loads(os.environ.get('TARGET_CONFIG'))
          migration_config = json.loads(os.environ.get('MIGRATION_CONFIG'))
          
          target_id = target_config.get('id')
          endpoint = target_config.get('endpoint')
          
          # Parse all RFS results
          all_rfs_results_str = os.environ.get('ALL_RFS_RESULTS')
          all_rfs_results = []
          
          try:
              # Try to parse as JSON array first
              parsed_results = json.loads(all_rfs_results_str)
              if isinstance(parsed_results, list):
                  for item in parsed_results:
                      if isinstance(item, dict):
                          all_rfs_results.append(item)
                      elif isinstance(item, str):
                          try:
                              all_rfs_results.append(json.loads(item))
                          except:
                              print(f"Warning: Could not parse RFS result item: {item[:50]}")
              elif isinstance(parsed_results, dict):
                  all_rfs_results.append(parsed_results)
          except Exception as e:
              print(f"Warning: Failed to parse RFS results as JSON: {str(e)}")
          
              # Try alternative parsing
              if all_rfs_results_str and all_rfs_results_str.startswith('[') and all_rfs_results_str.endswith(']'):
                  try:
                      import ast
                      parsed_list = ast.literal_eval(all_rfs_results_str)
          
                      if isinstance(parsed_list, list):
                          for item in parsed_list:
                              if isinstance(item, dict):
                                  all_rfs_results.append(item)
                              elif isinstance(item, str):
                                  try:
                                      all_rfs_results.append(json.loads(item))
                                  except:
                                      print(f"Warning: Could not parse RFS result item: {item[:50]}")
                  except Exception as e:
                      print(f"Warning: AST parsing failed: {str(e)}")
          
          # Filter RFS results for this target
          target_rfs_results = [r for r in all_rfs_results if r.get('target_id') == target_id]
          
          print(f"Aggregated {len(target_rfs_results)} RFS results for target {target_id}")
          
          # Group RFS results by source_id to apply correct replayer config
          source_indices = {}
          for rfs_result in target_rfs_results:
              source_id = rfs_result.get('source_id')
              index = rfs_result.get('index')
          
              if source_id not in source_indices:
                  source_indices[source_id] = set()
          
              source_indices[source_id].add(index)
          
          # Find replayer configs from migration config
          for source_id, indices in source_indices.items():
              print(f"Source {source_id} indices: {', '.join(indices)}")
          
              # Find replayer config for this source
              replayer_config = None
              for source_config in migration_config.get('source-migration-configurations', []):
                  if source_config.get('source', {}).get('id') == source_id:
                      replayer_config = source_config.get('replayer-config', {})
                      break
          
              if replayer_config:
                  batch_size = replayer_config.get('batchSize', 1000)
                  concurrency = replayer_config.get('concurrency', 4)
                  print(f"Replayer config for source {source_id}: batch size={batch_size}, concurrency={concurrency}")
              else:
                  print(f"No replayer config found for source {source_id}")
          
          print(f"Replay to target {target_id} completed successfully")
          
          # Write result to output file
          result = {
              "target_id": target_id,
              "status": "completed",
              "indices_processed": [result.get('index') for result in target_rfs_results]
          }
          with open('/tmp/result.json', 'w') as f:
              f.write(json.dumps(result))
      outputs:
        parameters:
          - name: result
            valueFrom:
              path: /tmp/result.json